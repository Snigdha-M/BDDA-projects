{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDDA Project 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snigdha Mathur (015050) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Performing Text Classification on the csv file \"Corona_NLP_train\" using PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have used Multinomial NaiveBayes for calculating the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing garbage collector for reducing the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new session for Spark with the name ProjNLP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ProjNLP2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"Corona_NLP_train.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file and taking a sample (70%) because the size of the file is too large to perform modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df.sample(fraction=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|            UserName|          ScreenName|            Location|             TweetAt|       OriginalTweet|         Sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|                3800|               48752|                  UK|          16-03-2020|advice Talk to yo...|          Positive|\n",
      "|                3801|               48753|           Vagabonds|          16-03-2020|Coronavirus Austr...|          Positive|\n",
      "|              PLEASE|         don't panic| THERE WILL BE EN...|                null|                null|              null|\n",
      "|           Stay calm|          stay safe.|                null|                null|                null|              null|\n",
      "|#COVID19france #C...|            Positive|                null|                null|                null|              null|\n",
      "|Not because I'm p...| but because my f...|          but please| don't panic. It ...|                null|              null|\n",
      "|                3804|               48756|ÃœT: 36.319708,-82...|          16-03-2020|As news of the re...|          Positive|\n",
      "|                3805|               48757|35.926541,-78.753267|          16-03-2020|\"Cashier at groce...|          Positive|\n",
      "|                3806|               48758|             Austria|          16-03-2020|Was at the superm...|              null|\n",
      "|#toiletpapercrisi...|             Neutral|                null|                null|                null|              null|\n",
      "|                3807|               48759|     Atlanta, GA USA|          16-03-2020|Due to COVID-19 o...|          Positive|\n",
      "|                3809|               48761|      Makati, Manila|          16-03-2020|All month there h...|           Neutral|\n",
      "|We thank you for ...|  Extremely Positive|                null|                null|                null|              null|\n",
      "|                3811|               48763|          Horningsea|          16-03-2020|#horningsea is a ...|Extremely Positive|\n",
      "|                3812|               48764|         Chicago, IL|          16-03-2020|Me: I don't need ...|              null|\n",
      "|Amazon: https://t...|            Positive|                null|                null|                null|              null|\n",
      "|                3813|               48765|                null|          16-03-2020|ADARA Releases CO...|          Positive|\n",
      "|Find out more abo...|                null|                null|                null|                null|              null|\n",
      "|                3815|               48767|        Saudi Arabia|          16-03-2020|????? ????? ?????...|              null|\n",
      "|#???_???????? ???...|             Neutral|                null|                null|                null|              null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47631"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depicting the count of tweets made per location and displaying top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Location|count|\n",
      "+--------------------+-----+\n",
      "|                 ...|    1|\n",
      "| Mumbai, Maharashtra|    4|\n",
      "| Brisbane, Australia|    8|\n",
      "|          South Asia|    1|\n",
      "|West Woofle-Dust ...|    1|\n",
      "|   St Petersburg, FL|   12|\n",
      "| All across Michigan|    1|\n",
      "|     Northumberland |    1|\n",
      "|     stoke on trent |    1|\n",
      "|   Dallas, Texas USA|    1|\n",
      "|some where around...|    1|\n",
      "|           Worcester|    3|\n",
      "|           Bangalore|   14|\n",
      "|            Novi, MI|    1|\n",
      "|Sagaon, Kalyan Do...|    1|\n",
      "|Ferrara, Emilia R...|    1|\n",
      "|      Luton, England|    2|\n",
      "|         black site |    1|\n",
      "| to all the gas s...|    1|\n",
      "|Just to the left ...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"Location\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the columns (tweet and sentiment) that are needed for prediction and text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df2.select(\"OriginalTweet\", \"Sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       OriginalTweet|         Sentiment|\n",
      "+--------------------+------------------+\n",
      "|advice Talk to yo...|          Positive|\n",
      "|Coronavirus Austr...|          Positive|\n",
      "|                null|              null|\n",
      "|                null|              null|\n",
      "|                null|              null|\n",
      "|                null|              null|\n",
      "|As news of the re...|          Positive|\n",
      "|\"Cashier at groce...|          Positive|\n",
      "|Was at the superm...|              null|\n",
      "|                null|              null|\n",
      "|Due to COVID-19 o...|          Positive|\n",
      "|All month there h...|           Neutral|\n",
      "|                null|              null|\n",
      "|#horningsea is a ...|Extremely Positive|\n",
      "|Me: I don't need ...|              null|\n",
      "|                null|              null|\n",
      "|ADARA Releases CO...|          Positive|\n",
      "|                null|              null|\n",
      "|????? ????? ?????...|              null|\n",
      "|                null|              null|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for NULL values in Sentiment and Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_count(df2):\n",
    "    null_columns_counts = []\n",
    "    numRows = df2.count()\n",
    "    for k in df2.columns:\n",
    "        nullRows = df2.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "            return(null_columns_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns_count_list = null_value_count(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the number of null vales in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------+\n",
      "|Column_With_Null_Value|Null_Values_Count|\n",
      "+----------------------+-----------------+\n",
      "|         OriginalTweet|            18712|\n",
      "+----------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       OriginalTweet|         Sentiment|\n",
      "+--------------------+------------------+\n",
      "|advice Talk to yo...|          Positive|\n",
      "|Coronavirus Austr...|          Positive|\n",
      "|As news of the re...|          Positive|\n",
      "|\"Cashier at groce...|          Positive|\n",
      "|Due to COVID-19 o...|          Positive|\n",
      "|All month there h...|           Neutral|\n",
      "|#horningsea is a ...|Extremely Positive|\n",
      "|ADARA Releases CO...|          Positive|\n",
      "|For those who are...|          Positive|\n",
      "|with 100  nations...|Extremely Negative|\n",
      "|In preparation fo...|          Negative|\n",
      "|This morning I te...|Extremely Negative|\n",
      "|Went to the super...|           Neutral|\n",
      "|Worried about the...|          Positive|\n",
      "|my wife works ret...|          Negative|\n",
      "|This is the line ...|           Neutral|\n",
      "| Please Share  Kn...|Extremely Positive|\n",
      "|\"\"\"Everything weÂ’...|            racism|\n",
      "|Why we stock up o...|          Negative|\n",
      "|Global food price...|Extremely Negative|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the count of tweets after dropping the null values from teh dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19991"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting text into raw words (tokenizing) and foeing a new column 'words' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"OriginalTweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized = regex_tokenizer.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|       OriginalTweet|         Sentiment|               words|\n",
      "+--------------------+------------------+--------------------+\n",
      "|@MeNyrbie @Phil_G...|           Neutral|[menyrbie, phil_g...|\n",
      "|\"Cashier at groce...|          Positive|[cashier, at, gro...|\n",
      "|For corona preven...|          Negative|[for, corona, pre...|\n",
      "|All month there h...|           Neutral|[all, month, ther...|\n",
      "|#horningsea is a ...|Extremely Positive|[horningsea, is, ...|\n",
      "|ADARA Releases CO...|          Positive|[adara, releases,...|\n",
      "|For those who are...|          Positive|[for, those, who,...|\n",
      "|with 100  nations...|Extremely Negative|[with, 100, natio...|\n",
      "|@10DowningStreet ...|          Negative|[10downingstreet,...|\n",
      "|UK #consumer poll...|Extremely Positive|[uk, consumer, po...|\n",
      "|In preparation fo...|          Negative|[in, preparation,...|\n",
      "|This morning I te...|Extremely Negative|[this, morning, i...|\n",
      "|Went to the super...|           Neutral|[went, to, the, s...|\n",
      "|my wife works ret...|          Negative|[my, wife, works,...|\n",
      "|Now I can go to t...|          Positive|[now, i, can, go,...|\n",
      "|Curious,  do we t...|          Positive|[curious, do, we,...|\n",
      "|CHECK VIDEO ?? ht...|Extremely Negative|[check, video, ht...|\n",
      "|This is the line ...|           Neutral|[this, is, the, l...|\n",
      "|Never thought I'd...|Extremely Positive|[never, thought, ...|\n",
      "|\"\"\"Everything weÂ’...|            racism|[everything, we, ...|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords from the raw words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "removed = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the filtered words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                                                                                                                                                                                                                                      |filtered                                                                                                                                                                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[menyrbie, phil_gahan, chrisitv, https, t, co, ifz9fan2pa, and, https, t, co, xx6ghgfzcc, and, https, t, co, i2nlzdxno8]                                                                                                                                                                                                                   |[menyrbie, phil_gahan, chrisitv, https, co, ifz9fan2pa, https, co, xx6ghgfzcc, https, co, i2nlzdxno8]                                                                                                                                                                                     |\n",
      "|[cashier, at, grocery, store, was, sharing, his, insights, on, covid_19, to, prove, his, credibility, he, commented, i, m, in, civics, class, so, i, know, what, i, m, talking, about, https, t, co, iefdnehgdo]                                                                                                                           |[cashier, grocery, store, sharing, insights, covid_19, prove, credibility, commented, m, civics, class, know, m, talking, https, co, iefdnehgdo]                                                                                                                                          |\n",
      "|[for, corona, prevention, we, should, stop, to, buy, things, with, the, cash, and, should, use, online, payment, methods, because, corona, can, spread, through, the, notes, also, we, should, prefer, online, shopping, from, our, home, it, s, time, to, fight, against, covid, 19, govindia, indiafightscorona]                         |[corona, prevention, stop, buy, things, cash, use, online, payment, methods, corona, spread, notes, also, prefer, online, shopping, home, time, fight, covid, 19, govindia, indiafightscorona]                                                                                            |\n",
      "|[all, month, there, hasn, t, been, crowding, in, the, supermarkets, or, restaurants, however, reducing, all, the, hours, and, closing, the, malls, means, everyone, is, now, using, the, same, entrance, and, dependent, on, a, single, supermarket, manila, lockdown, covid2019, philippines, https, t, co, hxws9lanf9]                   |[month, hasn, crowding, supermarkets, restaurants, however, reducing, hours, closing, malls, means, everyone, using, entrance, dependent, single, supermarket, manila, lockdown, covid2019, philippines, https, co, hxws9lanf9]                                                           |\n",
      "|[horningsea, is, a, caring, community, let, s, all, look, after, the, less, capable, in, our, village, and, ensure, they, stay, healthy, bringing, shopping, to, their, doors, help, with, online, shopping, and, self, isolation, if, you, have, symptoms, or, been, exposed, to, somebody, who, has, https, t, co, lsgrxxhjhh]           |[horningsea, caring, community, let, look, less, capable, village, ensure, stay, healthy, bringing, shopping, doors, help, online, shopping, self, isolation, symptoms, exposed, somebody, https, co, lsgrxxhjhh]                                                                         |\n",
      "|[adara, releases, covid, 19, resource, center, for, travel, brands, insights, help, travel, brands, stay, up, to, date, on, consumer, travel, behavior, trends, https, t, co, pna797jdkv, https, t, co, dqox6usihz]                                                                                                                        |[adara, releases, covid, 19, resource, center, travel, brands, insights, help, travel, brands, stay, date, consumer, travel, behavior, trends, https, co, pna797jdkv, https, co, dqox6usihz]                                                                                              |\n",
      "|[for, those, who, aren, t, struggling, please, consider, donating, to, a, food, bank, or, a, nonprofit, the, demand, for, these, services, will, increase, as, covid, 19, impacts, jobs, and, people, s, way, of, life]                                                                                                                    |[aren, struggling, please, consider, donating, food, bank, nonprofit, demand, services, increase, covid, 19, impacts, jobs, people, way, life]                                                                                                                                            |\n",
      "|[with, 100, nations, inficted, with, covid, 19, the, world, must, not, play, fair, with, china, 100, goverments, must, demand, china, adopts, new, guilde, lines, on, food, safty, the, chinese, goverment, is, guilty, of, being, irosponcible, with, life, on, a, global, scale]                                                         |[100, nations, inficted, covid, 19, world, must, play, fair, china, 100, goverments, must, demand, china, adopts, new, guilde, lines, food, safty, chinese, goverment, guilty, irosponcible, life, global, scale]                                                                         |\n",
      "|[10downingstreet, grantshapps, what, is, being, done, to, ensure, food, and, other, essential, products, are, being, re, stocked, at, supermarkets, and, panic, buying, actively, discouraged, it, cannot, be, left, to, checkout, staff, to, police, the, actions, of, the, selfish, and, profiteer]                                      |[10downingstreet, grantshapps, done, ensure, food, essential, products, re, stocked, supermarkets, panic, buying, actively, discouraged, left, checkout, staff, police, actions, selfish, profiteer]                                                                                      |\n",
      "|[uk, consumer, poll, indicates, the, majority, expect, covid19, s, impact, to, last, 4, 12, months, at, 12, march, we, expect, this, to, increase, at, the, next, tracker, see, full, results, of, the, retailx, coronavirus, consumer, confidence, tracker, here, https, t, co, k3ujlcjqdb, https, t, co, 9g3kgqixj8]                     |[uk, consumer, poll, indicates, majority, expect, covid19, impact, last, 4, 12, months, 12, march, expect, increase, next, tracker, see, full, results, retailx, coronavirus, consumer, confidence, tracker, https, co, k3ujlcjqdb, https, co, 9g3kgqixj8]                                |\n",
      "|[in, preparation, for, higher, demand, and, a, potential, food, shortage, the, hunger, coalition, purchased, 10, percent, more, food, and, implemented, new, protocols, due, to, the, covid, 19, coronavirus, https, t, co, 5cecytlnyn]                                                                                                    |[preparation, higher, demand, potential, food, shortage, hunger, coalition, purchased, 10, percent, food, implemented, new, protocols, due, covid, 19, coronavirus, https, co, 5cecytlnyn]                                                                                                |\n",
      "|[this, morning, i, tested, positive, for, covid, 19, i, feel, ok, i, have, no, symptoms, so, far, but, have, been, isolated, since, i, found, out, about, my, possible, exposure, to, the, virus, stay, home, people, and, be, pragmatic, i, will, keep, you, updated, on, how, i, m, doing, no, panic, https, t, co, lg7hvmzglz]          |[morning, tested, positive, covid, 19, feel, ok, symptoms, far, isolated, since, found, possible, exposure, virus, stay, home, people, pragmatic, keep, updated, m, panic, https, co, lg7hvmzglz]                                                                                         |\n",
      "|[went, to, the, supermarket, yesterday, and, the, toilet, paper, was, gone, has, this, anything, to, do, with, the, corona, virus, covid2019]                                                                                                                                                                                              |[went, supermarket, yesterday, toilet, paper, gone, anything, corona, virus, covid2019]                                                                                                                                                                                                   |\n",
      "|[my, wife, works, retail, amp, a, customer, came, in, yesterday, coughing, everywhere, saying, they, have, covid, 19, they, requested, a, deep, clean, of, the, store, her, company, objected, to, due, to, cost, recommending, the, team, spray, disinfectant, amp, clean, themselves, we, re, gonna, die, get, sick, due, to, capitalism]|[wife, works, retail, amp, customer, came, yesterday, coughing, everywhere, saying, covid, 19, requested, deep, clean, store, company, objected, due, cost, recommending, team, spray, disinfectant, amp, clean, re, gonna, die, get, sick, due, capitalism]                              |\n",
      "|[now, i, can, go, to, the, supermarket, like, this, without, being, judged, coronavirusoutbreak, covid2019, https, t, co, krtcgiuhqs]                                                                                                                                                                                                      |[go, supermarket, like, without, judged, coronavirusoutbreak, covid2019, https, co, krtcgiuhqs]                                                                                                                                                                                           |\n",
      "|[curious, do, we, think, retail, shoppers, will, do, a, lot, of, online, shopping, bc, they, re, home, and, unable, to, go, out, or, do, we, think, everyone, is, too, spooked, to, get, that, extra, pair, of, shoes, economy, onlineshopping, coronavirus, covid19, stayhome]                                                            |[curious, think, retail, shoppers, lot, online, shopping, bc, re, home, unable, go, think, everyone, spooked, get, extra, pair, shoes, economy, onlineshopping, coronavirus, covid19, stayhome]                                                                                           |\n",
      "|[check, video, https, t, co, 1ksn9brl02, no, food, in, usa, market, due, to, coronavirus, panic, we, gonna, die, from, starvation, coronavirusoutbreak, coronavirus, houston, nofood, notoiletpaper, nohandshakes, nohandsanitizer, covid19, pandemic, totallockdown, covid2019usa, walmart, https, t, co, ztn3imkgpd]                     |[check, video, https, co, 1ksn9brl02, food, usa, market, due, coronavirus, panic, gonna, die, starvation, coronavirusoutbreak, coronavirus, houston, nofood, notoiletpaper, nohandshakes, nohandsanitizer, covid19, pandemic, totallockdown, covid2019usa, walmart, https, co, ztn3imkgpd]|\n",
      "|[this, is, the, line, outside, target, in, as, customers, wait, for, the, store, to, open, this, morning]                                                                                                                                                                                                                                  |[line, outside, target, customers, wait, store, open, morning]                                                                                                                                                                                                                            |\n",
      "|[never, thought, i, d, say, this, but, 2019, will, you, come, back, please, coronavirus, covid19, peoplearelosingtheirminds, stopthemadness, stoppanicbuying]                                                                                                                                                                              |[never, thought, d, say, 2019, come, back, please, coronavirus, covid19, peoplearelosingtheirminds, stopthemadness, stoppanicbuying]                                                                                                                                                      |\n",
      "|[everything, we, re, seeing, in, the, current, covid, 19, outbreak, has, been, seen, before, in, previous, epidemics, and, pandemics, the, rise, of, fear]                                                                                                                                                                                 |[everything, re, seeing, current, covid, 19, outbreak, seen, previous, epidemics, pandemics, rise, fear]                                                                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removed.select(\"words\",\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding the Sentiment column into SentimentIndex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive- 0.0 <br>\n",
    "Negative - 1.0 <br>\n",
    "Neutral - 2.0 <br>\n",
    "Extremely Positive - 3.0 <br>\n",
    "Extremely Negative - 4.0 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"SentimentIndex\")\n",
    "feature_data = indexer.fit(removed).transform(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|         Sentiment|SentimentIndex|\n",
      "+------------------+--------------+\n",
      "|           Neutral|           2.0|\n",
      "|          Positive|           0.0|\n",
      "|          Negative|           1.0|\n",
      "|           Neutral|           2.0|\n",
      "|Extremely Positive|           3.0|\n",
      "|          Positive|           0.0|\n",
      "|          Positive|           0.0|\n",
      "|Extremely Negative|           4.0|\n",
      "|          Negative|           1.0|\n",
      "|Extremely Positive|           3.0|\n",
      "|          Negative|           1.0|\n",
      "|Extremely Negative|           4.0|\n",
      "|           Neutral|           2.0|\n",
      "|          Negative|           1.0|\n",
      "|          Positive|           0.0|\n",
      "|          Positive|           0.0|\n",
      "|Extremely Negative|           4.0|\n",
      "|           Neutral|           2.0|\n",
      "|Extremely Positive|           3.0|\n",
      "|            racism|         285.0|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_data.select(\"Sentiment\",\"SentimentIndex\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting text into vectors of token counts using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = cv.fit(feature_data)\n",
    "countVectorizer_feateures = model.transform(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train and test with 80% in train and rest 20% in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = countVectorizer_feateures.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Multinomial Naivebayes on the labelCol \"SentimentIndex\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"SentimentIndex\", featuresCol=\"features\")\n",
    "nbModel = nb.fit(trainingData)\n",
    "nb_predictions = nbModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the predicted vs the actual value of the sentiment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n",
      "|prediction|SentimentIndex|            features|\n",
      "+----------+--------------+--------------------+\n",
      "|       4.0|           4.0|(41642,[11,12,149...|\n",
      "|       0.0|           2.0|(41642,[2,10,829,...|\n",
      "|       1.0|           4.0|(41642,[5,19,21,8...|\n",
      "|       0.0|           2.0|(41642,[3,53,55,5...|\n",
      "|       1.0|           0.0|(41642,[8,24,26,8...|\n",
      "+----------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predictions.select(\"prediction\", \"SentimentIndex\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the accuracy of the model and displaying the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NaiveBayes is = 0.437672\n",
      "Test Error of NaiveBayes = 0.562328 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"SentimentIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
    "print(\"Test Error of NaiveBayes = %g \" % (1.0 - nb_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy comes out to be 43.7% and test error as 56.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This means that only 43.7% of the sentiments were identified correctly in the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
